# Design: Benchmark Metrics Tracker

## Summary

This design introduces a Python-based benchmark metrics tracker that compares Google Benchmark results against golden baseline files, detects performance regressions with configurable thresholds, and generates JSON comparison reports. The system enables automated performance regression detection for CI integration while providing detailed console output for local development workflows.

## Motivation

The Google Benchmark infrastructure (ticket 0011) provides performance measurements, but lacks automated regression detection. This tracker addresses that gap by:
1. Maintaining golden baseline files for performance expectations
2. Automatically comparing new results against baselines
3. Flagging regressions that exceed acceptable thresholds
4. Generating machine-readable comparison reports
5. Supporting CI integration via exit codes

## Architecture Overview

### Component Structure

```
scripts/
└── compare_benchmarks.py     # Main comparison script

benchmark_results/
└── {suite}/
    ├── benchmark_latest.json           # Generated by run_benchmarks.sh
    └── comparison_{timestamp}.json     # Generated by compare_benchmarks.py

benchmark_baselines/           # NEW directory (committed to git)
└── {suite}/
    └── baseline.json          # Golden baseline file
```

### Data Flow

```
1. User runs benchmarks:
   ./scripts/run_benchmarks.sh
   → Generates benchmark_results/{suite}/benchmark_latest.json

2. User compares results:
   ./scripts/compare_benchmarks.py
   → Reads benchmark_results/{suite}/benchmark_latest.json
   → Reads benchmark_baselines/{suite}/baseline.json
   → Compares metrics
   → Generates benchmark_results/{suite}/comparison_{timestamp}.json
   → Displays color-coded console output
   → Returns exit code (0=pass, 1=regression if --strict)

3. User updates baseline (when performance changes are expected):
   ./scripts/compare_benchmarks.py --set-baseline
   → Copies benchmark_latest.json to benchmark_baselines/{suite}/baseline.json
```

## Python Script Design

### Module: compare_benchmarks.py

**Location**: `scripts/compare_benchmarks.py`

**Language**: Python 3.9+

**Dependencies**: Standard library only
- `json` — Parse Google Benchmark JSON output
- `argparse` — Command-line interface
- `pathlib` — Path manipulation
- `sys` — Exit codes
- `datetime` — Timestamps for comparison reports

### Core Functions

#### 1. Benchmark Comparison Engine

```python
def compare_benchmarks(
    current: dict,
    baseline: dict,
    threshold_percent: float = 10.0
) -> dict:
    """
    Compare current benchmark results against baseline.

    Args:
        current: Google Benchmark JSON output (current run)
        baseline: Google Benchmark JSON output (baseline)
        threshold_percent: Regression threshold (default 10%)

    Returns:
        Comparison result dictionary with structure:
        {
            "metadata": {
                "current_date": "2026-01-08T14:30:00",
                "baseline_date": "2026-01-01T10:00:00",
                "threshold_percent": 10.0
            },
            "benchmarks": [
                {
                    "name": "BM_ConvexHull_Construction/8",
                    "current_cpu_time": 36833,
                    "baseline_cpu_time": 35000,
                    "diff_ns": 1833,
                    "diff_percent": 5.24,
                    "status": "PASS"  # or "REGRESSION"
                }
            ],
            "summary": {
                "total": 10,
                "passed": 9,
                "regressed": 1,
                "missing_in_current": [],
                "missing_in_baseline": ["BM_NewBenchmark"]
            }
        }
    """
    pass  # Implementation in implementation phase
```

#### 2. Benchmark Name Matching

```python
def match_benchmarks(
    current_benchmarks: list[dict],
    baseline_benchmarks: list[dict]
) -> tuple[list[tuple], list[str], list[str]]:
    """
    Match benchmarks by exact name, filtering out aggregates.

    Args:
        current_benchmarks: Benchmarks from current run
        baseline_benchmarks: Benchmarks from baseline

    Returns:
        Tuple of:
        - matched_pairs: [(current_bench, baseline_bench), ...]
        - missing_in_baseline: ["BM_NewBench", ...]
        - missing_in_current: ["BM_OldBench", ...]

    Note:
        Skips benchmarks with aggregate=True (BigO, RMS rows)
        Matches by exact "name" field (includes parameters like "/8")
    """
    pass  # Implementation in implementation phase
```

#### 3. Console Output

```python
def print_comparison_results(
    comparison: dict,
    color: bool = True
) -> None:
    """
    Print color-coded comparison results to console.

    Output format matches run_benchmarks.sh conventions:
    - GREEN: Performance improved or within threshold
    - YELLOW: Warning (new benchmarks, missing benchmarks)
    - RED: Regression detected

    Args:
        comparison: Result from compare_benchmarks()
        color: Enable ANSI color codes (default True)
    """
    pass  # Implementation in implementation phase
```

#### 4. Baseline Management

```python
def set_baseline(
    results_file: Path,
    baseline_file: Path
) -> None:
    """
    Copy current results to baseline file.

    Creates baseline directory if it doesn't exist.
    Validates that results_file exists and contains valid JSON.

    Args:
        results_file: Path to benchmark_latest.json
        baseline_file: Path to baseline.json destination

    Raises:
        FileNotFoundError: If results_file doesn't exist
        ValueError: If results_file contains invalid JSON
    """
    pass  # Implementation in implementation phase
```

### Command-Line Interface

```bash
# Compare latest results against baseline (default behavior)
./scripts/compare_benchmarks.py

# Compare specific result file
./scripts/compare_benchmarks.py --current benchmark_results/msd_sim_bench/benchmark_20260108.json

# Compare with custom threshold
./scripts/compare_benchmarks.py --threshold 5.0

# Set current results as new baseline
./scripts/compare_benchmarks.py --set-baseline

# Strict mode: exit code 1 on regression (for CI)
./scripts/compare_benchmarks.py --strict

# Specify benchmark suite
./scripts/compare_benchmarks.py --suite msd_sim_bench

# Output JSON only (no console output)
./scripts/compare_benchmarks.py --output-json-only

# Disable colors
./scripts/compare_benchmarks.py --no-color
```

### CLI Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--current` | Path | `benchmark_latest.json` | Path to current benchmark results |
| `--baseline` | Path | Auto-detected | Path to baseline file (auto: `benchmark_baselines/{suite}/baseline.json`) |
| `--suite` | str | Auto-detected | Benchmark suite name (auto-detect from current file path) |
| `--threshold` | float | 10.0 | Regression threshold percentage |
| `--set-baseline` | flag | False | Copy current results to baseline |
| `--strict` | flag | False | Exit code 1 on regression |
| `--output-json-only` | flag | False | Only write JSON report, no console output |
| `--no-color` | flag | False | Disable ANSI color codes |

## Data Structures

### Google Benchmark JSON Schema (Input)

```json
{
  "context": {
    "date": "2026-01-08T14:30:00-05:00",
    "executable": "./build/Release/release/msd_sim_bench",
    "num_cpus": 8,
    "mhz_per_cpu": 2400,
    "cpu_scaling_enabled": false,
    "library_build_type": "release"
  },
  "benchmarks": [
    {
      "name": "BM_ConvexHull_Construction/8",
      "family_index": 0,
      "per_family_instance_index": 0,
      "run_name": "BM_ConvexHull_Construction/8",
      "run_type": "iteration",
      "repetitions": 3,
      "repetition_index": 0,
      "threads": 1,
      "iterations": 4174,
      "real_time": 36974,
      "cpu_time": 36833,
      "time_unit": "ns"
    },
    {
      "name": "BM_ConvexHull_Construction_BigO",
      "family_index": 0,
      "per_family_instance_index": 0,
      "run_name": "BM_ConvexHull_Construction",
      "run_type": "aggregate",
      "repetitions": 0,
      "threads": 1,
      "aggregate_name": "BigO",
      "aggregate_unit": "N",
      "cpu_time": 260.09,
      "time_unit": "ns"
    }
  ]
}
```

### Comparison Report Schema (Output)

```json
{
  "metadata": {
    "generated_at": "2026-01-08T15:00:00",
    "current_date": "2026-01-08T14:30:00",
    "baseline_date": "2026-01-01T10:00:00",
    "threshold_percent": 10.0,
    "suite": "msd_sim_bench"
  },
  "benchmarks": [
    {
      "name": "BM_ConvexHull_Construction/8",
      "current_cpu_time": 36833,
      "baseline_cpu_time": 35000,
      "diff_ns": 1833,
      "diff_percent": 5.24,
      "status": "PASS",
      "current_iterations": 4174,
      "baseline_iterations": 4500
    },
    {
      "name": "BM_ConvexHull_Contains",
      "current_cpu_time": 2500,
      "baseline_cpu_time": 2000,
      "diff_ns": 500,
      "diff_percent": 25.0,
      "status": "REGRESSION",
      "current_iterations": 64000,
      "baseline_iterations": 70000
    }
  ],
  "summary": {
    "total_compared": 10,
    "passed": 9,
    "regressed": 1,
    "missing_in_current": [],
    "missing_in_baseline": ["BM_NewBenchmark/1024"]
  }
}
```

## Directory Structure Changes

### New Directories

```
benchmark_baselines/           # NEW - Committed to git
└── msd_sim_bench/
    └── baseline.json
```

### Modified Files

**`.gitignore`** — Ensure `benchmark_baselines/` is NOT ignored
```gitignore
# Benchmark results are NOT committed
benchmark_results/

# Benchmark baselines ARE committed (golden files)
# benchmark_baselines/ <-- should NOT be in .gitignore
```

## Console Output Design

### Color Scheme (Matching run_benchmarks.sh)

```bash
# Color codes (ANSI)
GREEN='\033[0;32m'   # Pass, improvement
YELLOW='\033[1;33m'  # Warning, new benchmarks
RED='\033[0;31m'     # Regression
BLUE='\033[0;34m'    # Info
NC='\033[0m'         # No color

# Example output
Comparing benchmarks against baseline...
Suite: msd_sim_bench
Threshold: 10.0%

┌────────────────────────────────────┬─────────────┬─────────────┬──────────┬────────┐
│ Benchmark                          │ Current     │ Baseline    │ Diff     │ Status │
├────────────────────────────────────┼─────────────┼─────────────┼──────────┼────────┤
│ BM_ConvexHull_Construction/8       │ 36833 ns    │ 35000 ns    │ +5.24%   │ PASS   │  # GREEN
│ BM_ConvexHull_Construction/64      │ 93823 ns    │ 92000 ns    │ +1.98%   │ PASS   │  # GREEN
│ BM_ConvexHull_Contains             │ 2500 ns     │ 2000 ns     │ +25.0%   │ REGR   │  # RED
└────────────────────────────────────┴─────────────┴─────────────┴──────────┴────────┘

Summary:
  Total: 10
  Passed: 9 (GREEN)
  Regressed: 1 (RED)

New benchmarks (not in baseline):
  - BM_NewBenchmark/1024 (YELLOW)

Comparison report: benchmark_results/msd_sim_bench/comparison_20260108_150000.json
```

## Error Handling

### Error Scenarios

| Error | Exit Code | User Action |
|-------|-----------|-------------|
| No baseline file exists | 1 | Run `--set-baseline` to create initial baseline |
| Current results file not found | 1 | Run `./scripts/run_benchmarks.sh` first |
| Invalid JSON in baseline | 1 | Restore baseline from git or regenerate |
| Invalid JSON in current results | 1 | Re-run benchmarks |
| Regression detected (--strict mode) | 1 | Investigate performance regression or update baseline |
| Regression detected (default mode) | 0 | Warning printed, but exit 0 for local workflows |

### Error Messages

```python
# Missing baseline
ERROR: Baseline file not found: benchmark_baselines/msd_sim_bench/baseline.json
To create a baseline, run:
  ./scripts/compare_benchmarks.py --set-baseline

# Missing results
ERROR: Current results file not found: benchmark_results/msd_sim_bench/benchmark_latest.json
Run benchmarks first:
  ./scripts/run_benchmarks.sh

# Invalid JSON
ERROR: Invalid JSON in baseline file: benchmark_baselines/msd_sim_bench/baseline.json
Check git history or regenerate baseline.

# Regression in strict mode
ERROR: Performance regression detected (--strict mode)
1 benchmark(s) exceeded threshold of 10.0%
See comparison report for details.
```

## Integration with Existing Infrastructure

### run_benchmarks.sh Integration

No modifications required to `run_benchmarks.sh`. The comparison script consumes its output.

**Workflow**:
```bash
# Developer workflow
./scripts/run_benchmarks.sh                      # Generate results
./scripts/compare_benchmarks.py                  # Compare against baseline

# Update baseline when performance changes are expected
./scripts/run_benchmarks.sh
./scripts/compare_benchmarks.py --set-baseline
git add benchmark_baselines/
git commit -m "Update benchmark baseline after optimization"

# CI workflow (future enhancement)
./scripts/run_benchmarks.sh
./scripts/compare_benchmarks.py --strict         # Fail CI on regression
```

### Git Workflow

**Baseline files are committed** to ensure team-wide consistency:
```bash
# Developer makes optimization
git checkout -b perf-optimization

# Verify improvement
./scripts/run_benchmarks.sh
./scripts/compare_benchmarks.py
# Output shows improvement

# Update baseline
./scripts/compare_benchmarks.py --set-baseline

# Commit both code and updated baseline
git add src/optimization.cpp
git add benchmark_baselines/msd_sim_bench/baseline.json
git commit -m "Optimize ConvexHull construction

Performance improvement:
- BM_ConvexHull_Construction/512: 253ms → 180ms (-28%)

Updated benchmark baseline."
```

## Testing Strategy

Since this is a Python utility script (not production C++ code), testing approach:

### Manual Testing Checklist

| Test Case | Expected Result |
|-----------|-----------------|
| Run without baseline file | Error message with instructions |
| Run `--set-baseline` | Baseline file created in correct location |
| Run with identical results | All PASS, 0% difference |
| Run with 5% slower results, 10% threshold | All PASS |
| Run with 15% slower results, 10% threshold | REGRESSION flagged |
| Run with `--strict` and regression | Exit code 1 |
| Run without `--strict` and regression | Exit code 0, warning displayed |
| Run with new benchmark (not in baseline) | Yellow warning, reported in "new benchmarks" |
| Run with missing benchmark (in baseline, not current) | Yellow warning, reported in "missing in current" |
| Verify JSON report structure | Valid JSON, matches schema |
| Verify color output | GREEN/YELLOW/RED as expected |
| Run with `--no-color` | No ANSI codes in output |

### Future Enhancement: pytest Tests

When time permits, add unit tests:
- `tests/test_compare_benchmarks.py` — Unit tests for core functions
- Mock JSON fixtures for various regression scenarios
- Test edge cases (empty benchmarks, malformed JSON)

## Performance Characteristics

### Expected Performance

- **File I/O**: Two JSON files read, one JSON file written
- **Memory**: Load both JSON files into memory (~100KB typical)
- **CPU**: Simple percentage calculations on benchmark arrays
- **Total execution time**: < 1 second for typical benchmark suites (10-50 benchmarks)

### Scalability

The comparison is O(n*m) where n=current benchmarks, m=baseline benchmarks, but:
- Benchmark suites typically small (< 100 benchmarks)
- Simple dictionary lookups dominate
- No complex statistical analysis in v1

For large suites (>1000 benchmarks), could optimize with dict-based matching, but not needed for current scale.

## Documentation Updates

### CLAUDE.md Changes

**Section to add**: "Benchmark Regression Detection"

Location: After "Generating Benchmark Reports" in Benchmarking section

```markdown
### Benchmark Regression Detection

**Ticket**: [0014_benchmark_metrics_tracker](tickets/0014_benchmark_metrics_tracker.md)
**Design**: [`docs/designs/0014_benchmark_metrics_tracker/design.md`](docs/designs/0014_benchmark_metrics_tracker/design.md)

The project uses `compare_benchmarks.py` to detect performance regressions by comparing results against golden baseline files.

**Basic workflow**:
```bash
# Run benchmarks
./scripts/run_benchmarks.sh

# Compare against baseline
./scripts/compare_benchmarks.py

# Update baseline (when performance changes are intentional)
./scripts/compare_benchmarks.py --set-baseline
```

**Interpreting results**:
- **GREEN**: Performance within threshold or improved
- **YELLOW**: New/missing benchmarks (review if expected)
- **RED**: Regression detected (exceeds threshold)

**Default threshold**: 10% slower than baseline triggers regression

**CI integration**:
```bash
# Fail CI pipeline on regression
./scripts/compare_benchmarks.py --strict
```

**Comparison reports**:
- Location: `benchmark_results/{suite}/comparison_{timestamp}.json`
- Format: JSON with per-benchmark diff, summary statistics
- Useful for: Design review, pull request analysis

**Baseline files**:
- Location: `benchmark_baselines/{suite}/baseline.json`
- Committed to git for team-wide consistency
- Update when intentional performance changes occur
```

## Open Questions

### Design Decisions (Human Input Needed)

None — All design decisions were provided in the ticket's "Design Decisions" section.

### Prototype Required

None — Straightforward Python script with well-defined inputs/outputs and no algorithmic uncertainty.

### Requirements Clarification

None — Requirements are clear and comprehensive.

## Implementation Checklist

### Files to Create
- [ ] `scripts/compare_benchmarks.py` — Main script
- [ ] `benchmark_baselines/` — Directory for baseline files
- [ ] `benchmark_baselines/msd_sim_bench/baseline.json` — Initial baseline (copy from latest results)

### Files to Modify
- [ ] `.gitignore` — Ensure `benchmark_baselines/` is NOT ignored (remove if present)
- [ ] `CLAUDE.md` — Add "Benchmark Regression Detection" section

### Implementation Tasks
1. [ ] Implement JSON parsing for Google Benchmark format
2. [ ] Implement benchmark name matching (exact match, skip aggregates)
3. [ ] Implement percentage difference calculation
4. [ ] Implement regression detection with configurable threshold
5. [ ] Implement comparison report JSON generation
6. [ ] Implement color-coded console output
7. [ ] Implement CLI argument parsing
8. [ ] Implement `--set-baseline` functionality
9. [ ] Implement `--strict` mode exit code logic
10. [ ] Add executable permissions to script
11. [ ] Create initial baseline file from current benchmark results
12. [ ] Update CLAUDE.md documentation
13. [ ] Manual testing with various scenarios

## Success Criteria

The implementation is complete when:
1. All Acceptance Criteria in the ticket are met
2. Script successfully detects regressions in test scenarios
3. Console output matches color scheme conventions
4. JSON reports validate against defined schema
5. `--set-baseline` creates baseline files in correct location
6. Baseline files are committed to git (not gitignored)
7. CLAUDE.md documentation is updated
8. Manual testing checklist is complete

---

## Design Review

**Reviewer**: Design Review Agent
**Date**: 2026-01-08
**Status**: APPROVED
**Iteration**: 0 of 1 (no revision needed)

### Criteria Assessment

#### Architectural Fit

| Criterion | Pass/Fail | Notes |
|-----------|-----------|-------|
| Integration with existing infrastructure | ✓ | Additive only - no modifications to `run_benchmarks.sh` or benchmark executables |
| Directory structure | ✓ | Follows existing conventions: `scripts/`, `benchmark_results/`, new `benchmark_baselines/` |
| Naming conventions | ✓ | Script name `compare_benchmarks.py` matches bash script pattern `run_benchmarks.sh` |
| Color scheme consistency | ✓ | Uses RED/GREEN/YELLOW matching existing `run_benchmarks.sh` (verified via script examination) |
| Git workflow integration | ✓ | Baseline files committed, results files gitignored - clear separation |

#### Python Script Quality

| Criterion | Pass/Fail | Notes |
|-----------|-----------|-------|
| Dependency management | ✓ | stdlib only (json, argparse, pathlib, sys, datetime) - no pip dependencies |
| Python version requirement | ✓ | Python 3.9+ with modern generics (`dict`, `list` not `Dict`, `List`) |
| CLI design | ✓ | Comprehensive argparse interface with sensible defaults and clear help text |
| Error handling | ✓ | Well-defined error scenarios with actionable messages and appropriate exit codes |
| Data structure design | ✓ | Clear JSON schemas for input/output, proper metadata tracking |

#### Feasibility

| Criterion | Pass/Fail | Notes |
|-----------|-----------|-------|
| Implementation complexity | ✓ | Straightforward Python script - JSON parsing, percentage calculations, console output |
| Google Benchmark JSON schema | ✓ | Verified compatibility with actual output from existing benchmarks |
| Performance requirements | ✓ | < 1 second execution time achievable with simple O(n*m) matching for typical scale |
| File I/O strategy | ✓ | Simple read/write operations, no complex filesystem interactions |
| Testability | ✓ | Manual testing checklist comprehensive, pytest option for future enhancement |

#### Integration Points

| Criterion | Pass/Fail | Notes |
|-----------|-----------|-------|
| Consumes existing output | ✓ | Reads `benchmark_latest.json` produced by `run_benchmarks.sh` |
| No modification of producers | ✓ | Design explicitly avoids changing benchmark infrastructure |
| CI integration readiness | ✓ | `--strict` flag provides exit code 1 on regression for CI workflows |
| Human workflow support | ✓ | Default mode (exit 0 on regression) suitable for local development |

### Risks Identified

| ID | Risk Description | Category | Likelihood | Impact | Mitigation | Prototype? |
|----|------------------|----------|------------|--------|------------|------------|
| R1 | Aggregate rows (BigO, RMS) incorrectly included in comparison | Technical | Low | Low | Design explicitly filters `run_type == "aggregate"` or checks for aggregate fields | No |
| R2 | Parameterized benchmark name matching fails for complex patterns | Technical | Low | Medium | Design uses exact name matching (e.g., `BM_ConvexHull_Construction/8`) which aligns with Google Benchmark output | No |
| R3 | Color codes interfere with CI log parsers | Integration | Low | Low | `--no-color` flag provided, CI can disable colors if needed | No |
| R4 | JSON schema changes in Google Benchmark updates | Maintenance | Low | Medium | Design uses only stable fields (`name`, `cpu_time`, `run_type`), version field available for validation | No |
| R5 | Baseline drift over time on different hardware | Operational | Medium | Low | Documented in design - baselines are machine-specific, teams should establish baseline update process | No |

### Prototype Guidance

**No prototypes required** - All identified risks are low-likelihood/low-impact, and the design provides clear mitigation strategies. The implementation is straightforward with well-understood technologies (Python stdlib, JSON parsing, file I/O).

### Design Strengths

1. **Clear separation of concerns**: Comparison script is independent of benchmark generation, enabling future extensibility
2. **Comprehensive error handling**: Every error scenario has clear messaging and actionable resolution
3. **Thoughtful CLI design**: Sensible defaults for common use, extensive options for power users
4. **Well-defined data structures**: JSON schemas documented with examples matching real output
5. **Integration-first approach**: Design explicitly considers existing infrastructure and avoids breaking changes
6. **Documentation-focused**: CLAUDE.md update section included in design, manual testing checklist provided
7. **CI-ready with local-friendly defaults**: `--strict` for CI, exit 0 by default for local workflows

### Minor Recommendations

These are suggestions for implementation, not blocking issues:

1. **Console output formatting**: Consider using Python's `tabulate` module if added as dependency in future (currently correctly avoided per constraints)
2. **Timestamp formatting**: Ensure ISO 8601 format consistency across all timestamp fields in comparison reports
3. **JSON validation**: Consider adding a `--validate-json` flag for debugging malformed benchmark output
4. **Verbose mode**: A `--verbose` or `-v` flag could help with debugging comparison logic if issues arise

### Summary

This design is **approved without revision**. The benchmark metrics tracker is well-architected with:
- Clear requirements and comprehensive specifications
- Proper integration with existing infrastructure (additive only)
- Thoughtful error handling and user experience design
- No high-impact risks requiring prototyping
- Ready for direct implementation

The design demonstrates strong understanding of the existing benchmark infrastructure (ticket 0011) and follows project conventions for directory structure, naming, and color schemes. The Python-only approach with stdlib dependencies aligns with the stated constraints and keeps the tool lightweight and portable.

**Next Steps**: Proceed directly to implementation phase. No prototypes required. The manual testing checklist provides sufficient validation coverage for a utility script of this scope.
